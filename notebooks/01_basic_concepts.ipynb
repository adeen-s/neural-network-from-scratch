{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks from Scratch - Basic Concepts\n",
    "\n",
    "This notebook introduces the basic concepts of neural networks and demonstrates how to use our from-scratch implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.core.network import NeuralNetwork\n",
    "from src.core.layers import Dense\n",
    "from src.core.activations import ReLU, Sigmoid, Softmax\n",
    "from src.core.losses import MeanSquaredError, CategoricalCrossentropy\n",
    "from src.core.optimizers import SGD, Adam\n",
    "from src.datasets.synthetic import make_classification, make_circles\n",
    "from src.utils.data_loader import to_categorical\n",
    "from src.utils.metrics import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Neural Network Components\n",
    "\n",
    "### Dense Layers\n",
    "Dense (fully connected) layers are the building blocks of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dense layer\n",
    "layer = Dense(input_size=3, output_size=2)\n",
    "print(f\"Weights shape: {layer.weights.shape}\")\n",
    "print(f\"Biases shape: {layer.biases.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "input_data = np.array([1.0, 2.0, 3.0])\n",
    "output = layer.forward(input_data)\n",
    "print(f\"Output: {output.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "Activation functions introduce non-linearity to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "relu = ReLU()\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "y_relu = [relu.forward(np.array([[xi]]))[0, 0] for xi in x]\n",
    "plt.plot(x, y_relu, label='ReLU')\n",
    "plt.title('ReLU Activation')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "y_sigmoid = [sigmoid.forward(np.array([[xi]]))[0, 0] for xi in x]\n",
    "plt.plot(x, y_sigmoid, label='Sigmoid')\n",
    "plt.title('Sigmoid Activation')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Your First Neural Network\n",
    "\n",
    "Let's solve the classic XOR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_xor = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "print(\"XOR Truth Table:\")\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\"{X_xor[i]} -> {y_xor[i][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train network\n",
    "network = NeuralNetwork()\n",
    "network.add(Dense(2, 4))\n",
    "network.add(ReLU())\n",
    "network.add(Dense(4, 1))\n",
    "network.add(Sigmoid())\n",
    "\n",
    "network.compile(\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    loss=MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = network.fit(X_xor, y_xor, epochs=1000, verbose=False)\n",
    "\n",
    "# Test\n",
    "predictions = network.predict(X_xor)\n",
    "print(\"\\nResults:\")\n",
    "for i in range(len(X_xor)):\n",
    "    pred = predictions[i][0]\n",
    "    actual = y_xor[i][0]\n",
    "    print(f\"{X_xor[i]} -> Predicted: {pred:.4f}, Actual: {actual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification with Synthetic Data\n",
    "\n",
    "Let's try a more complex classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y = make_circles(n_samples=200, noise=0.1, random_state=42)\n",
    "\n",
    "# Visualize data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "plt.title('Circles Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train network for circles\n",
    "network_circles = NeuralNetwork()\n",
    "network_circles.add(Dense(2, 8))\n",
    "network_circles.add(ReLU())\n",
    "network_circles.add(Dense(8, 4))\n",
    "network_circles.add(ReLU())\n",
    "network_circles.add(Dense(4, 1))\n",
    "network_circles.add(Sigmoid())\n",
    "\n",
    "network_circles.compile(\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    loss=MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Train\n",
    "history_circles = network_circles.fit(X, y.reshape(-1, 1), epochs=300, verbose=False)\n",
    "\n",
    "# Evaluate\n",
    "predictions_circles = network_circles.predict(X)\n",
    "acc = accuracy(y, (predictions_circles > 0.5).astype(int))\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key Takeaways\n",
    "\n",
    "1. **Dense layers** perform linear transformations\n",
    "2. **Activation functions** add non-linearity\n",
    "3. **Loss functions** measure prediction quality\n",
    "4. **Optimizers** update network parameters\n",
    "5. **Training** is an iterative process of forward and backward passes\n",
    "\n",
    "In the next notebook, we'll explore more advanced topics and build deeper networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
